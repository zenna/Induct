% TLP2esam.tex / sample pages for TLP
% v2.11, released 6-nov-2002

\documentclass{tlp}
\usepackage{aopmath}

\begin{document}
\bibliographystyle{acmtrans}

\title{The RL Super-Learning Bot-Machine-Program}

\author[Z. Tavares, M. Siegel]
{ZENNA TAVARES \\
MIT\\
E-mail: zenna@mit.edu
}

% \author[M. SIEGEL]
% {SIEGEL MAX \\
% MIT\\
% E-mail: maxs@mit.edu
% }

\pagerange{\pageref{firstpage}--\pageref{lastpage}}

\maketitle

\label{firstpage}

\begin{abstract}
%
Boom!

\end{abstract}

The idea is to simultaneously learn a model and use that model to plan in some set of domains.
We frame this as a planning problem in simultaneous domains: $\mathcal{D}_{world}$, $\mathcal{D}_{model}$ and $\mathcal{D}_{action}$.

The model is represented as a probabilistic program P we can sample new world states from.
That is, a model is a random variable from a current state, an action to apply, and some random input to a new state and associated reward.
A model is then:

\[
P : \Omega \times \mathcal{S} \times A_{world} \to \mathcal{S} \times R
\]

\subsection{General Algorithm}
We will consider discrete time domains, and may or may not evolve independently of the agents' action.

At each time step the agent will 1. update its model of the environment 2. use that model to potentially select an action.

\subsection{Using the model}


\subsection{Building a model}
Models are built by applying syntax transformations to the model.
A model is evaluated by how well it performs in the 


Suppose the set of all type-consistent well formed programs defines a language $\mathcal{L}$.
We have an initial set of actions $A_{init}$ which are syntactic transformations on $P$. I.e. for any $a \in A_{init}$,

\[
a: \mathcal{L} \to \mathcal{L}
\]




\subsection{Domain}
We wil use the Arcade Learning Environment

\bibliography{tlp2esam}

\end{document}



